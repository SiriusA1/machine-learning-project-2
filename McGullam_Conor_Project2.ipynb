{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6253090",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829ce6b",
   "metadata": {},
   "source": [
    "### Question 1: non DNN based classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d26d3",
   "metadata": {},
   "source": [
    "Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fcea5",
   "metadata": {},
   "source": [
    "The decision tree is a greedy learner that we are using as a basic starting point to improve upon in the next 3 models. The decision tree is created by asking \"questions\" with which to split the data at each node. The questions involve the features, which in this case are the 784 pixels of the image. The pros of this model are that it's very easy to implement and interpret, it works well with our large data set (60000 images), and it doesn't require any special preparation of the data. The cons are that it is the least accurate of the 4 models we'll use due to its tendency to overfit and it is non-robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf0a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mnist import MNIST\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import log\n",
    "from math import e\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334489a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('samples')\n",
    "\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "images_test, labels_test = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f945fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_test_dict = {}\n",
    "cols_list = []\n",
    "target_dict = {'labels': labels}\n",
    "target_test_dict = {'labels': labels_test}\n",
    "for c in range(len(images[0])):\n",
    "    cols_list.append(\"Pixel \" + str(c))\n",
    "for i in range(len(images)):\n",
    "    data_dict[i] = images[i]\n",
    "for j in range(len(images_test)):\n",
    "    data_test_dict[j] = images_test[j]\n",
    "#each row in dataframe is an image, each column represents a pixel\n",
    "data = pd.DataFrame.from_dict(data_dict, orient='index', columns=cols_list)\n",
    "data_test = pd.DataFrame.from_dict(data_test_dict, orient='index', columns=cols_list)\n",
    "target = pd.DataFrame.from_dict(target_dict)\n",
    "target_test = pd.DataFrame.from_dict(target_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c0ad66f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 912    1    7    9    5   10   13    5   10    8]\n",
      " [   1 1092    7    6    1    7    5    4   10    2]\n",
      " [  11   14  880   34   10   12   12   23   27    9]\n",
      " [   6    5   31  858    6   43    5   10   22   24]\n",
      " [   6    3   11    8  856    9   16   10   20   43]\n",
      " [  16    7    6   43    8  745   21    4   23   19]\n",
      " [  15    5   11    8   16   19  850    0   28    6]\n",
      " [   3   11   24   16    9    5    3  930    7   20]\n",
      " [  10    5   32   34   24   36   14   11  785   23]\n",
      " [  16    3   10   22   42   12    8   19   18  859]]\n",
      "87.67\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(data, target)\n",
    "target_pred = dt.predict(data_test)\n",
    "cmdtree = confusion_matrix(target_test, target_pred)\n",
    "print(cmdtree)\n",
    "DTC_accuracy = dt.score(data_test, target_test)*100\n",
    "print(DTC_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd19a6",
   "metadata": {},
   "source": [
    "Our accuracy with the decision tree is acceptable, but we can do better. From the confusion matrix we can see the algorithm tended to confuse 4's and 9's, 3's and 8's, and 3's and 5's. This is expected as those numbers have similar shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecb49c",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3f54d",
   "metadata": {},
   "source": [
    "Random forests are an ensemble classifier/learning method that uses a collection of different decision trees in along with group voting to classify samples. The decision trees are created using a random subset of the features. This way, each decision tree is slightly different and you can eliminate certain overfitting errors that a single decision tree with all features would have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15bf6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 971    0    1    0    0    2    3    1    2    0]\n",
      " [   0 1125    3    2    0    1    2    0    1    1]\n",
      " [   8    0  991    8    3    0    1    9   12    0]\n",
      " [   0    0   10  971    0    6    0    7   12    4]\n",
      " [   1    0    3    0  954    0    4    1    2   17]\n",
      " [   5    0    1   17    2  853    8    1    4    1]\n",
      " [   8    3    0    0    4    3  939    0    1    0]\n",
      " [   1    6   21    2    2    0    0  978    1   17]\n",
      " [   5    0    3    9    5    7    4    5  930    6]\n",
      " [   7    5    2   10    9    3    1    5    8  959]]\n",
      "96.71\n"
     ]
    }
   ],
   "source": [
    "#Random Forest original paper suggests log2(N+1) max features. Since each image has 784 pixels, we use log(784+1, 2)\n",
    "rf = RandomForestClassifier(max_features=int(log(784+1, 2)))\n",
    "rf.fit(data, target)\n",
    "target_pred = rf.predict(data_test)\n",
    "cmdtree = confusion_matrix(target_test, target_pred)\n",
    "print(cmdtree)\n",
    "RFC_accuracy = rf.score(data_test, target_test)*100\n",
    "print(RFC_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f946621",
   "metadata": {},
   "source": [
    "As expected, our random forest accuracy is significantly better than the decision tree accuracy. We can see in the confusion matrix that 4's and 9's, 3's and 8's, and 3's and 5's are still being mixed up the most, but to a much lesser extent than before. We also see 2's and 7's being mixed up, but this those numbers have similar shapes as well so it's not unusual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428076c",
   "metadata": {},
   "source": [
    "### Question 2: Neural Network & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b1cf6",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5a10d",
   "metadata": {},
   "source": [
    "The multilayer perceptron that I will use is a type of neural network classifier that consists of multiple layers. The first layer is the input layer which takes in the data, the last layer is the ouput layer which gives the classifications, and inbetween are multiple \"hidden layers\" which perform various activation functions. Each layer is made of multiple neurons or nodes. There are no hard and fast rules for the number of layers and the layer sizes, but since our data is not linearly separable we should use multiple hidden layers for best results. I've read that the best rule for hidden layer sizes is that it should be somewhere between the sizes of the input and output layers (in this case 10-784), so I chose 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a4be987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 966    1    2    0    0    1    4    2    3    1]\n",
      " [   1 1122    2    1    0    1    3    1    4    0]\n",
      " [   1    0 1006    4    2    0    2    9    7    1]\n",
      " [   0    0    5  985    0   11    0    7    1    1]\n",
      " [   1    0    4    0  959    0    2    3    2   11]\n",
      " [   3    0    0    6    1  869    3    3    5    2]\n",
      " [   3    3    3    0    9   10  923    0    7    0]\n",
      " [   0    2    5    2    1    0    1 1012    2    3]\n",
      " [   1    0    3    4    2    4    3    3  952    2]\n",
      " [   1    3    2    4   14    4    0   12    4  965]]\n",
      "97.59\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=300).fit(data, target)\n",
    "target_pred = mlp.predict(data_test)\n",
    "mlptree = confusion_matrix(target_test, target_pred)\n",
    "print(mlptree)\n",
    "MLP_accuracy = mlp.score(data_test, target_test)*100\n",
    "print(MLP_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5625492",
   "metadata": {},
   "source": [
    "For MLP, our accuracy does not seem to be much better than a much simpler and faster random forest. It's possible that I didn't tune that attributes of the classifier correctly, but I think it's also possible that we might see diminishing returns at this level of accuracy. Regardless of the model used, we're seeing the same errors appear. For instance, there's still a large number of 8's being read as 3's by the algorithm. Since those numbers are already close in shape, it's possible that certain samples are written too sloppily/indistinguishably for the machine to classify them accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b822",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83c0bc",
   "metadata": {},
   "source": [
    "For deep learning we will be using a convolutional neural network. The layers in a CNN convolve the image and pass the convolved image to the next layer as input. Convolution is done through kernels, which essentially pass over the image, manipulating the data within the \"window\" of pixels, and creating a pixel on the convolved image with the output of the kernel. This is repeated until the kernel gets through every pixel of the original image. CNN is strong for classification, but it's difficult to understand exactly what is going on in the convolutional layers, so the models are harder to tune is something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8da69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def get_weights(shape):\n",
    "    data = tf.compat.v1.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(data)\n",
    "\n",
    "def get_biases(shape):\n",
    "    data = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(data)\n",
    "\n",
    "def create_layer(shape):\n",
    "    # Get the weights and biases\n",
    "    W = get_weights(shape)\n",
    "    b = get_biases([shape[-1]])\n",
    "\n",
    "    return W, b\n",
    "\n",
    "def convolution_2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],\n",
    "            padding='SAME')\n",
    "\n",
    "def max_pooling(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def CNN(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    # The images are 28x28. Create the input layer\n",
    "    x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "    # Reshape 'x' into a 4D tensor\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # Define the first convolutional layer\n",
    "    W_conv1, b_conv1 = create_layer([5, 5, 1, 32])\n",
    "\n",
    "    # Convolve the image with weight tensor, add the\n",
    "    # bias, and then apply the ReLU function\n",
    "    h_conv1 = tf.nn.relu(convolution_2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    # Apply the max pooling operator\n",
    "    h_pool1 = max_pooling(h_conv1)\n",
    "\n",
    "    # Define the second convolutional layer\n",
    "    W_conv2, b_conv2 = create_layer([5, 5, 32, 64])\n",
    "\n",
    "    # Convolve the output of previous layer with the\n",
    "    # weight tensor, add the bias, and then apply\n",
    "    # the ReLU function\n",
    "    h_conv2 = tf.nn.relu(convolution_2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    # Apply the max pooling operator\n",
    "    h_pool2 = max_pooling(h_conv2)\n",
    "\n",
    "    # Define the fully connected layer\n",
    "    W_fc1, b_fc1 = create_layer([7*7*64, 1024])\n",
    "\n",
    "    # Reshape the output of the previous layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "    # Multiply the output of previous layer by the\n",
    "    # weight tensor, add the bias, and then apply\n",
    "    # the ReLU function * Use \"tf.matmul\" for matrix multiplication\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Define the dropout layer using a probability placeholder\n",
    "    # for all the neurons\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Define the readout layer (output layer)\n",
    "    #changed 10 to 1, couldn't get code working\n",
    "    W_fc2, b_fc2 = create_layer([1024, 1])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # Define the entropy loss and the optimizer\n",
    "    #changed 10 to 1, couldn't get code working\n",
    "    y_loss = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_loss))\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Define the accuracy computation\n",
    "    predicted = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_loss, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted, tf.float32))\n",
    "\n",
    "    # Create and run a session\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "    init = tf.compat.v1.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start training\n",
    "    batch_size = 75\n",
    "    num_iterations = math.ceil(X_train.shape[0] / batch_size)\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    print('\\nTraining the model....')\n",
    "    for i in range(num_iterations):\n",
    "        batch = (X_train[start:end], Y_train[start:end])\n",
    "        start = end\n",
    "        end = min(end + batch_size, X_train.shape[0])\n",
    "\n",
    "        # Print progress\n",
    "        if i % 50 == 0:\n",
    "            cur_accuracy = accuracy.eval(feed_dict = {\n",
    "                    x: batch[0], y_loss: batch[1], keep_prob: 1.0})\n",
    "            print('Iteration', i, ', Accuracy =', cur_accuracy)\n",
    "\n",
    "        # Train on the current batch\n",
    "        optimizer.run(feed_dict = {x: batch[0], y_loss: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    # Compute accuracy using test data\n",
    "    test_accuracy = accuracy.eval(feed_dict = {\n",
    "            x: X_test, y_loss: Y_test,\n",
    "            keep_prob: 1.0})\n",
    "    print('Test accuracy =', test_accuracy)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8585d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model....\n",
      "Iteration 0 , Accuracy = 1.0\n",
      "Iteration 50 , Accuracy = 1.0\n",
      "Iteration 100 , Accuracy = 1.0\n",
      "Iteration 150 , Accuracy = 1.0\n",
      "Iteration 200 , Accuracy = 1.0\n",
      "Iteration 250 , Accuracy = 1.0\n",
      "Iteration 300 , Accuracy = 1.0\n",
      "Iteration 350 , Accuracy = 1.0\n",
      "Iteration 400 , Accuracy = 1.0\n",
      "Iteration 450 , Accuracy = 1.0\n",
      "Iteration 500 , Accuracy = 1.0\n",
      "Iteration 550 , Accuracy = 1.0\n",
      "Iteration 600 , Accuracy = 1.0\n",
      "Iteration 650 , Accuracy = 1.0\n",
      "Iteration 700 , Accuracy = 1.0\n",
      "Iteration 750 , Accuracy = 1.0\n",
      "Test accuracy = 1.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "CNN_accuracy = CNN(data, target, data_test, target_test)*100\n",
    "print(CNN_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a372c",
   "metadata": {},
   "source": [
    "Unfortunely the accuracy for CNN is incorrect as I could not get the algorithm to properly accept the Y_train and Y_test dataframes that I had. It's likely the proper accuracy would have been higher than the MLP classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90df7ea",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c903991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85a1b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_labels = [8, 8, 8, 8, 8, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0]\n",
    "samples = []\n",
    "for f in os.listdir(\"my_mnist\"):\n",
    "    samples.append(np.ndarray.flatten(np.asarray(ImageOps.invert(Image.open(os.path.join(\"my_mnist\",f))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ab67f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_dict = {}\n",
    "my_target_dict = {'labels': my_labels}\n",
    "for i in range(len(samples)):\n",
    "    #thresholding to get rid of the noise in the image\n",
    "    sample_trans = [0 if a_ < 100 else a_ for a_ in samples[i]]\n",
    "    my_data_dict[i] = sample_trans\n",
    "my_data = pd.DataFrame.from_dict(my_data_dict, orient='index', columns=cols_list)\n",
    "my_target = pd.DataFrame.from_dict(my_target_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d6e13",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87c863df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 2 1 0 0]\n",
      " [0 1 1 1 0 0 0 1 1 0]\n",
      " [0 0 3 1 0 0 0 1 0 0]\n",
      " [0 1 0 1 0 1 0 1 1 0]\n",
      " [0 2 0 0 0 0 2 1 0 0]\n",
      " [1 1 0 0 0 3 0 0 0 0]\n",
      " [1 0 2 0 0 2 0 0 0 0]\n",
      " [0 0 1 1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 1 0 0 2 0]\n",
      " [0 0 1 0 1 0 1 1 0 1]]\n",
      "28.000000000000004\n"
     ]
    }
   ],
   "source": [
    "target_pred = dt.predict(my_data)\n",
    "cmdtree = confusion_matrix(my_target, target_pred)\n",
    "print(cmdtree)\n",
    "DTC_accuracy2 = dt.score(my_data, my_target)*100\n",
    "print(DTC_accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813bee1",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ebf6dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 2 0 0 1 0 0 0 0]\n",
      " [0 4 0 0 0 1 0 0 0 0]\n",
      " [0 0 5 0 0 0 0 0 0 0]\n",
      " [0 0 1 4 0 0 0 0 0 0]\n",
      " [0 1 0 0 3 0 0 1 0 0]\n",
      " [0 1 0 0 0 4 0 0 0 0]\n",
      " [0 0 1 0 1 2 1 0 0 0]\n",
      " [0 4 0 0 0 0 0 1 0 0]\n",
      " [0 4 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 2 0 0 1 0 1]]\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "target_pred = rf.predict(my_data)\n",
    "cmdtree = confusion_matrix(my_target, target_pred)\n",
    "print(cmdtree)\n",
    "RFC_accuracy2 = rf.score(my_data, my_target)*100\n",
    "print(RFC_accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd759314",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89c61d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 0 0 0 0 0 0 1 0]\n",
      " [0 2 0 1 0 2 0 0 0 0]\n",
      " [0 0 5 0 0 0 0 0 0 0]\n",
      " [0 0 0 5 0 0 0 0 0 0]\n",
      " [0 0 0 0 4 0 0 0 1 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 1 0]\n",
      " [0 0 1 0 0 0 0 4 0 0]\n",
      " [0 0 0 1 0 0 0 0 4 0]\n",
      " [0 0 0 0 1 0 0 1 0 3]]\n",
      "80.0\n"
     ]
    }
   ],
   "source": [
    "target_pred = mlp.predict(my_data)\n",
    "mlptree = confusion_matrix(my_target, target_pred)\n",
    "print(mlptree)\n",
    "MLP_accuracy2 = mlp.score(my_data, my_target)*100\n",
    "print(MLP_accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf7b5a",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fa19014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model....\n",
      "Iteration 0 , Accuracy = 1.0\n",
      "Iteration 50 , Accuracy = 1.0\n",
      "Iteration 100 , Accuracy = 1.0\n",
      "Iteration 150 , Accuracy = 1.0\n",
      "Iteration 200 , Accuracy = 1.0\n",
      "Iteration 250 , Accuracy = 1.0\n",
      "Iteration 300 , Accuracy = 1.0\n",
      "Iteration 350 , Accuracy = 1.0\n",
      "Iteration 400 , Accuracy = 1.0\n",
      "Iteration 450 , Accuracy = 1.0\n",
      "Iteration 500 , Accuracy = 1.0\n",
      "Iteration 550 , Accuracy = 1.0\n",
      "Iteration 600 , Accuracy = 1.0\n",
      "Iteration 650 , Accuracy = 1.0\n",
      "Iteration 700 , Accuracy = 1.0\n",
      "Iteration 750 , Accuracy = 1.0\n",
      "Test accuracy = 1.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "CNN_accuracy2 = CNN(data, target, my_data, my_target)*100\n",
    "print(CNN_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3177b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result1 = {'Model Number': [1,2,3,4], 'Algorithm(s)':['Decision Tree','Random Forest','Multilayer Perceptron','CNN'],\n",
    "           'Accuracy':[DTC_accuracy,RFC_accuracy,MLP_accuracy,CNN_accuracy]}\n",
    "\n",
    "Result1 = pd.DataFrame(Result1, columns = ['Model Number', 'Algorithm(s)','Accuracy'])\n",
    "Result2 = {'Model Number': [1,2,3,4], 'Algorithm(s)':['Decision Tree','Random Forest','Multilayer Perceptron','CNN'],\n",
    "           'Accuracy':[DTC_accuracy2,RFC_accuracy2,MLP_accuracy2,CNN_accuracy2]}\n",
    "\n",
    "Result2 = pd.DataFrame(Result2, columns = ['Model Number', 'Algorithm(s)','Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7284fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Data: \n",
      "   Model Number           Algorithm(s)  Accuracy\n",
      "0             1          Decision Tree     87.67\n",
      "1             2          Random Forest     96.71\n",
      "2             3  Multilayer Perceptron     97.59\n",
      "3             4                    CNN    100.00\n",
      "My Handwritten Data: \n",
      "   Model Number           Algorithm(s)  Accuracy\n",
      "0             1          Decision Tree      28.0\n",
      "1             2          Random Forest      50.0\n",
      "2             3  Multilayer Perceptron      80.0\n",
      "3             4                    CNN     100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST Data: \")\n",
    "print(Result1)\n",
    "print(\"My Handwritten Data: \")\n",
    "print(Result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73bfae",
   "metadata": {},
   "source": [
    "As we can see, the accuracy for the handwritten dataset is generally lower than for the MNIST dataset. This is likely due to a couple of things. One, the process that I used to write and scale the images is likely different from what the original creators of the MNIST set used. My process involves more noise and imperfection. The shadows on the paper that I wrote showed up after scaling and tweaking the color levels and added random color to the background of the image, whereas the MNIST images are completely white everywhere that there isn't a  number. I used some thresholding to eliminate most of the shadows, but some were still present. Also, having to scale the image down to 28x28 pixels is a very imperfect, messy process and a lot of the color and detail of my original handwriting is lost. Having smaller sample images drastically cuts down on the training times, but can reduce the quality of the classifier. The second reason for the lower accuracies is that the first two models, DT and RF, are prone to overfitting. This is why the MLP and CNN (if it had worked properly) accuracies are much higher as they are less prone to overfitting. The third reason for lower accuracy is the small set. With only 50 images, any errors in handwriting or testing can skew the accuracy a lot more than in a testing set of 10000 images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
